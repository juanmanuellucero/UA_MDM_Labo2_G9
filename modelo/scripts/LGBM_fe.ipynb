{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 🔹 2. Setup base\n",
    "\t1.\tLeer train.csv.\n",
    "\t2.\tUsar stratified split (80/20) o CV estratificado en 5 folds por AdoptionSpeed.\n",
    "\t3.\tDefinir un set de features básicas (categorías, numéricas, sin texto por ahora).\n",
    "\t4.\tEntrenar un modelo LightGBM multiclass con parámetros por defecto.\n",
    "\t5.\tEvaluar con cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo datos estructurados\n",
    "Este notebook desarrolla un primer modelo para resolver el problema de Petfinder. Empezamos haciendo un modelo inicial muy simple para ver la viabilidad de resolver el problema. Luego analizamos como se comporta la métrica kappa propuesta y vemos la matriz de confusión. Finalmente hacemos una optimización de hiperparametros evaluando con train/test y otra validando con 5 fold CV y testeando en el 20% de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m express \u001b[38;5;28;01mas\u001b[39;00m px\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#Plot de matriz de confusion normalizada en actuals\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutiles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#Optimizacion de hiperparametros\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utiles'"
     ]
    }
   ],
   "source": [
    "#Import de librerias basicas tablas y matrices\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Funciones auxiliares sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold #Split y cross Validation\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score #Metricas\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "#Visualizacióon\n",
    "from plotly import express as px\n",
    "\n",
    "#Plot de matriz de confusion normalizada en actuals\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "#Optimizacion de hiperparametros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "#Guardado de objetos en archivos joblib\n",
    "from joblib import load, dump\n",
    "\n",
    "\n",
    "workDir = \"../../work/\"\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "if not os.path.exists(workDir):\n",
    "    os.makedirs(workDir)\n",
    "\n",
    "if not os.path.exists(optunaArtifactDir):\n",
    "    os.makedirs(optunaArtifactDir)\n",
    "\n",
    "if not os.path.exists(optunaTempDir):\n",
    "    os.makedirs(optunaTempDir)\n",
    "    \n",
    "os.chdir(\n",
    "    os.path.join(\n",
    "        os.getcwd(), workDir)\n",
    "    )\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "# Paths para acceso archivos\n",
    "#Este notebook asume la siguiente estructura de carpetas a partir de la ubicacion de base_dir \n",
    "#(dos niveles arriba de la cƒarpeta donde se ejecuta el notebook). \n",
    "# /ƒ/ƒ\n",
    "# /UA_MDM_Labo2/inputƒ\n",
    "# /UA_MDM_Labo2/input/petfinder-adoption-prediction/            <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# /UA_MDM_Labo2/tutoriales/                       <- Aca deben poner los notebooks y scripts que les compartimos\n",
    "# /UA_MDM_Labo2/work/                             <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# /UA_MDM_Labo2/work/models/                     <- Modelos entrenados en archivos joblibs\n",
    "# /UA_MDM_Labo2/work/optuna_temp_artifacts/      <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# /UA_MDM_Labo2/work/optuna_artifacts/           <- Archivos con artefactos que sibimos a optuna\n",
    "\n",
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "TEST_SIZE = 0.2 #Facción para train/test= split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos Tabulares\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columnas del dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de features numericas\n",
    "numeric_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defino features a usar en un primer modelo de prueba\n",
    "features = ['Type',\n",
    " 'Age',\n",
    " 'Breed1',\n",
    " 'Breed2',\n",
    " 'Gender',\n",
    " 'Color1',\n",
    " 'Color2',\n",
    " 'Color3',\n",
    " 'MaturitySize',\n",
    " 'FurLength',\n",
    " 'Vaccinated',\n",
    " 'Dewormed',\n",
    " 'Sterilized',\n",
    " 'Health',\n",
    " 'Quantity',\n",
    " 'Fee',\n",
    " 'State',\n",
    " 'VideoAmt',\n",
    " 'PhotoAmt']\n",
    "\n",
    "label = 'AdoptionSpeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoión\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "cohen_kappa_score(y_test,y_pred, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestro la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebo un modelo alternativo donde en vez de usar la version multiclass real de lightGBM utilizo One vs All\n",
    "\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclassova',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAtriz de confusion y Kappa dfe OVA\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "display(plot_confusion_matrix(y_test,y_pred))\n",
    "\n",
    "{'kappa':cohen_kappa_score(y_test,\n",
    "                y_pred,\n",
    "                weights = 'quadratic'),\n",
    " 'accuracy':accuracy_score(y_test,y_pred),\n",
    " 'balanced_accuracy':balanced_accuracy_score(y_test,y_pred)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizacion de hiperparametros modelo train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Funcion que vamos a optimizar. Optuna requiere que usemos el objeto trial para generar los parametros a optimizar\n",
    "def lgb_objective(trial):\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Genero objeto dataset de entrenamiento\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                    label=y_train)\n",
    "\n",
    "    #ajuste de modelo\n",
    "    lgb_model = lgb.train(lgb_params,\n",
    "                        lgb_train_dataset)\n",
    "    \n",
    "    #Devuelvo el score en test\n",
    "    return(cohen_kappa_score(y_test,lgb_model.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100 - LGB Multiclass_10\", #nombre del experimento\n",
    "                            load_if_exists=True) #continuar si ya existe\n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor solución que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique())} | study.best_params\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "display(plot_confusion_matrix(y_test,lgb_model.predict(X_test).argmax(axis=1)))\n",
    "\n",
    "cohen_kappa_score(y_test,lgb_model.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con cross validation y conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero una metrica para que lightGBM haga la evaluación y pueda hacer early_stopping en el cross validation\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(),dy_pred.argmax(axis=1),weights = 'quadratic')\n",
    "    is_higher_better = True\n",
    "    return(metric_name, value, is_higher_better)\n",
    "\n",
    "#Funcion objetivo a optimizar. En este caso vamos a hacer 5fold cv sobre el conjunto de train. \n",
    "# El score de CV es el objetivo a optimizar. Ademas vamos a usar los 5 modelos del CV para estimar el conjunto de test,\n",
    "# registraremos en optuna las predicciones, matriz de confusion y el score en test.\n",
    "# CV Score -> Se usa para determinar el rendimiento de los hiperparametros con precision \n",
    "# Test Score -> Nos permite testear que esta todo OK, no use (ni debo usar) esos datos para nada en el entrenamiento \n",
    "# o la optimizacion de hiperparametros\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Voy a generar estimaciones de los 5 modelos del CV sobre los datos test y los acumulo en la matriz scores_ensemble\n",
    "    scores_ensemble = np.zeros((len(y_test),len(y_train.unique())))\n",
    "\n",
    "    #Score del 5 fold CV inicializado en 0\n",
    "    score_folds = 0\n",
    "\n",
    "    #Numero de splits del CV\n",
    "    n_splits = 5\n",
    "\n",
    "    #Objeto para hacer el split estratificado de CV\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        \n",
    "        #Dataset in fold (donde entreno) \n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                        label=y_train.iloc[if_index],\n",
    "                                        free_raw_data=False)\n",
    "        \n",
    "        #Dataset Out of fold (donde mido la performance del CV)\n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                        label=y_train.iloc[oof_index],\n",
    "                                        free_raw_data=False)\n",
    "\n",
    "        #Entreno el modelo\n",
    "        lgb_model = lgb.train(lgb_params,\n",
    "                                lgb_if_dataset,\n",
    "                                valid_sets=lgb_oof_dataset,\n",
    "                                callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "                                feval = lgb_custom_metric_kappa\n",
    "                                )\n",
    "        \n",
    "        #Acumulo los scores (probabilidades) de cada clase para cada uno de los modelos que determino en los folds\n",
    "        #Se predice el 20% de los datos que separe para tes y no uso para entrenar en ningun fold\n",
    "        scores_ensemble = scores_ensemble + lgb_model.predict(X_test)\n",
    "        \n",
    "        #Score del fold (registros de dataset train que en este fold quedan out of fold)\n",
    "        score_folds = score_folds + cohen_kappa_score(y_train.iloc[oof_index], \n",
    "                                                            lgb_model.predict(X_train.iloc[oof_index]).argmax(axis=1),weights = 'quadratic')/n_splits\n",
    "\n",
    "\n",
    "    #Guardo prediccion del trial sobre el conjunto de test\n",
    "    # Genero nombre de archivo\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    # Copia del dataset para guardar la prediccion\n",
    "    predicted_df = test.copy()\n",
    "    # Genero columna pred con predicciones sumadas de los 5 folds\n",
    "    predicted_df['pred'] = [scores_ensemble[p,:] for p in range(scores_ensemble.shape[0])]\n",
    "    # Grabo dataframe en temp_artifacts\n",
    "    dump(predicted_df, predicted_filename)\n",
    "    # Indico a optuna que asocie el archivo generado al trial\n",
    "    upload_artifact(trial, predicted_filename, artifact_store)    \n",
    "\n",
    "    #Grabo natriz de confusion\n",
    "    #Nombre de archivo\n",
    "    cm_filename = os.path.join(PATH_TO_TEMP_FILES,f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "    #Grabo archivo\n",
    "    plot_confusion_matrix(y_test,scores_ensemble.argmax(axis=1)).write_image(cm_filename)\n",
    "    #Asocio al trial\n",
    "    upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "    #Determino score en conjunto de test y asocio como metrica adicional en optuna\n",
    "    test_score = cohen_kappa_score(y_test,scores_ensemble.argmax(axis=1),weights = 'quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    #Devuelvo score del 5fold cv a optuna para que optimice en base a eso\n",
    "    return(score_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicio el store de artefactos (archivos) de optuna\n",
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100 - LGB Multiclass CV_10\",\n",
    "                            load_if_exists = True)\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver el optuna dashboard tengo que correr este comando en la terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde la terminal\n",
    "Para cambiar el directorio \n",
    "-  cd '/Users/ri1965/Desktop/Maestria DS Austral 2024/11 - Labo 02/work'\n",
    "\n",
    "Para correr Optuna\n",
    "- optuna-dashboard sqlite:////Users/ri1965/Desktop/Maestria\\ DS\\ Austral\\ 2024/11\\ -\\ Labo\\ 02/work/db.sqlite3 --artifact-dir /Users/ri1965/Desktop/Maestria\\ DS\\ Austral\\ 2024/11\\ -\\ Labo\\ 02/work/optuna_artifacts --port 8081\n",
    "\n",
    "Esta era la linea oficial de la cátedra\n",
    "- #!optuna-dashboard sqlite:///../work/db.sqlite3 --artifact-dir ../work/optuna_artifacts --port 8081"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
