{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c37b40",
   "metadata": {},
   "source": [
    "## Configuracion de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3edd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si est√°s en Jupyter Notebook, ancl√°s a una ruta conocida o fija:\n",
    "project_root = r\"C:\\Users\\juanm\\GitHub\\UA_MDM_Labo2_G9\"  # ‚ö†Ô∏è Personaliz√° esto\n",
    "\n",
    "# Alternativa: usar una carpeta conocida dentro del proyecto\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Menos fr√°gil\n",
    "\n",
    "# Rutas de trabajo\n",
    "workDir = os.path.join(project_root, \"work\")\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(optunaArtifactDir, exist_ok=True)\n",
    "os.makedirs(optunaTempDir, exist_ok=True)\n",
    "\n",
    "# Cambiar al directorio de trabajo solo si es necesario\n",
    "if os.getcwd() != workDir:\n",
    "    os.chdir(workDir)\n",
    "\n",
    "print(\"Directorio actual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulaci√≥n de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelado: Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Sklearn: splits, m√©tricas y utilidades\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt  # Usar junto con seaborn o plotly si es necesario\n",
    "import plotly.express as px      # Correcci√≥n: no usar `from plotly import express as px`\n",
    "\n",
    "# Optuna para optimizaci√≥n de hiperpar√°metros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "# Utilidades propias\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "# Guardado de objetos\n",
    "from joblib import dump, load\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b908605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths para acceso archivos\n",
    "#Este notebook asume la siguiente estructura de carpetas a partir de la ubicacion de base_dir \n",
    "#(dos niveles arriba de la c∆íarpeta donde se ejecuta el notebook). \n",
    "# /∆í/∆í\n",
    "# /UA_MDM_Labo2/input∆í\n",
    "# /UA_MDM_Labo2/input/petfinder-adoption-prediction/            <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# /UA_MDM_Labo2/tutoriales/                       <- Aca deben poner los notebooks y scripts que les compartimos\n",
    "# /UA_MDM_Labo2/work/                             <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# /UA_MDM_Labo2/work/models/                     <- Modelos entrenados en archivos joblibs\n",
    "# /UA_MDM_Labo2/work/optuna_temp_artifacts/      <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# /UA_MDM_Labo2/work/optuna_artifacts/           <- Archivos con artefactos que sibimos a optuna\n",
    "\n",
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Datos de razas \n",
    "PATH_TO_BREED_LABELS = os.path.join(BASE_DIR, \"modelo/data/petfinder-adoption-prediction/BreedLabels.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "TEST_SIZE = 0.2 #Facci√≥n para train/test= split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba176f",
   "metadata": {},
   "source": [
    "## Descripcion Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f0781",
   "metadata": {},
   "source": [
    "Con base en el script actualizado de ingenier√≠a de caracter√≠sticas que me compartiste, te actualizo la descripci√≥n de las transformaciones del dataset para que est√©n alineadas con las variables efectivamente generadas:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. **Transformaciones directas √∫tiles**\n",
    "\n",
    "Variables originales reformuladas para aportar m√°s valor predictivo al modelo.\n",
    "\n",
    "| Variable     | Qu√© hace                                                                  | Por qu√© ayuda                                                           |\n",
    "| ------------ | ------------------------------------------------------------------------- | ----------------------------------------------------------------------- |\n",
    "| `HasName`    | Indica si el animal tiene nombre                                          | Un nombre puede hacer al animal m√°s cercano y memorable para adoptantes |\n",
    "| `PureBreed`  | Indica si el animal es de raza pura (`Breed2 == 0` y no contiene \"Mixed\") | Ayuda a capturar la preferencia por razas espec√≠ficas                   |\n",
    "| `DescLength` | Longitud de la descripci√≥n textual                                        | M√°s texto puede reflejar mayor esfuerzo y generar m√°s inter√©s           |\n",
    "| `Fee_log`    | (No generado en el script, pero se puede incluir) Logaritmo del precio    | Suaviza la distribuci√≥n y reduce el impacto de valores extremos         |\n",
    "| `IsFree`     | (Sugerido) `Fee == 0`                                                     | La gratuidad puede aumentar la intenci√≥n de adopci√≥n                    |\n",
    "\n",
    "**Objetivo**: transformar variables originales en formas m√°s robustas o informativas para el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. **Reagrupaciones o codificaciones √∫tiles**\n",
    "\n",
    "Nuevas variables creadas mediante categorizaci√≥n o codificaci√≥n combinada.\n",
    "\n",
    "| Variable           | Qu√© hace                                                                 | Por qu√© ayuda                                             |\n",
    "| ------------------ | ------------------------------------------------------------------------ | --------------------------------------------------------- |\n",
    "| `Type_Breed_Combo` | Combina el tipo de animal con el ID de la raza principal                 | Algunas combinaciones pueden ser m√°s atractivas que otras |\n",
    "| `Fee_Breed_Ratio`  | Relaci√≥n entre la tarifa individual y el promedio de tarifa para su raza | Detecta casos con precio inusualmente alto o bajo         |\n",
    "\n",
    "**Objetivo**: facilitar el aprendizaje del modelo mediante agrupaciones con patrones compartidos.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. **Interacciones entre variables**\n",
    "\n",
    "Variables creadas a partir de combinaciones que expresan relaciones no lineales.\n",
    "\n",
    "| Variable               | Qu√© hace                                                  | Por qu√© ayuda                                       |\n",
    "| ---------------------- | --------------------------------------------------------- | --------------------------------------------------- |\n",
    "| `Fee_per_Pet`          | Tarifa dividida por cantidad de mascotas                  | Normaliza la tarifa para adopciones m√∫ltiples       |\n",
    "| `Photo_per_Pet`        | Cantidad de fotos por mascota                             | Mide el esfuerzo visual por cada animal             |\n",
    "| `Age_Fee_Ratio`        | Edad dividida por tarifa (m√°s 1)                          | Detecta casos inusuales de edad vs. valor monetario |\n",
    "| `DescLength_per_Pet`   | Longitud de descripci√≥n dividida por cantidad de mascotas | Indica esfuerzo textual por cada animal             |\n",
    "| `DescLength_per_Photo` | Longitud de descripci√≥n dividida por cantidad de fotos    | Relaciona el detalle textual con el esfuerzo visual |\n",
    "\n",
    "**Objetivo**: capturar relaciones entre variables que puedan influir en decisiones de adopci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e967956",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd789d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# 1. Tiene nombre\n",
    "dataset['HasName'] = dataset['Name'].notnull().astype(int)\n",
    "\n",
    "# 2. Es de raza pura\n",
    "# Cargar etiquetas de razas\n",
    "breed_labels = pd.read_csv(PATH_TO_BREED_LABELS)\n",
    "\n",
    "# Diccionario ID ‚Üí Nombre de raza\n",
    "id_to_breed = dict(zip(breed_labels['BreedID'], breed_labels['BreedName']))\n",
    "\n",
    "# Crear una nueva columna con el nombre de Breed1\n",
    "dataset['Breed1_name'] = dataset['Breed1'].map(id_to_breed)\n",
    "\n",
    "dataset['PureBreed'] = (\n",
    "    (dataset['Breed2'] == 0) &\n",
    "    (~dataset['Breed1_name'].str.contains('Mixed', case=False, na=False))\n",
    ").astype(int)\n",
    "\n",
    "# 3. Longitud de la descripci√≥n\n",
    "dataset['DescLength'] = dataset['Description'].fillna('').apply(len)\n",
    "\n",
    "# 4. Combinaci√≥n tipo-raza principal\n",
    "dataset['Type_Breed_Combo'] = dataset['Type'].astype(str) + '_' + dataset['Breed1'].astype(str)\n",
    "\n",
    "dataset['Type_Breed_Combo'] = dataset['Type_Breed_Combo'].astype('category')\n",
    "\n",
    "# 5. Tarifa por mascota\n",
    "dataset['Fee_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['Fee'] / dataset['Quantity'])\n",
    "\n",
    "# 6. Fotos por mascota\n",
    "dataset['Photo_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['PhotoAmt'] / dataset['Quantity'])\n",
    "\n",
    "# 7. Relaci√≥n edad/tarifa\n",
    "dataset['Age_Fee_Ratio'] = dataset['Age'] / (dataset['Fee'] + 1)\n",
    "\n",
    "# 8. Longitud descripci√≥n por mascota\n",
    "dataset['DescLength_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['DescLength'] / dataset['Quantity'])\n",
    "\n",
    "# 9. Longitud descripci√≥n por cantidad de fotos \n",
    "dataset['DescLength_per_Photo'] = np.where(dataset['PhotoAmt'] == 0,0,dataset['DescLength'] / dataset['PhotoAmt'])\n",
    "\n",
    "# 10. Tarifa promedio por raza\n",
    "breed_fee_mean = dataset.groupby('Breed1')['Fee'].mean()\n",
    "\n",
    "dataset['Fee_Breed_Ratio'] = dataset['Fee'] / (dataset['Breed1'].map(breed_fee_mean) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)\n",
    "\n",
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']\n",
    "\n",
    "#Defino features a usar\n",
    "features = numeric_feats.copy()\n",
    "\n",
    "label = 'AdoptionSpeed'\n",
    "\n",
    "# Eliminamos 'AdoptionSpeed' si est√°\n",
    "if 'AdoptionSpeed' in features:\n",
    "    features.remove('AdoptionSpeed')\n",
    "\n",
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca5d89",
   "metadata": {},
   "source": [
    "## Modelo con hiperparametros por default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cacb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoi√≥n\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model1 = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model1.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0683698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model1.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "print(cohen_kappa_score(y_test,y_pred, weights = 'quadratic'))\n",
    "\n",
    "#Muestro la matriz de confusi√≥n\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270bbba",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que vamos a optimizar. Optuna requiere que usemos el objeto trial para generar los parametros a optimizar\n",
    "def lgb_objective(trial):\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Genero objeto dataset de entrenamiento\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                    label=y_train)\n",
    "\n",
    "    #ajuste de modelo\n",
    "    lgb_model2 = lgb.train(lgb_params,\n",
    "                        lgb_train_dataset)\n",
    "    \n",
    "    #Devuelvo el score en test\n",
    "    return(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_JM_30.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_FE_JM_30\", #nombre del experimento\n",
    "                            load_if_exists=True) #continuar si ya existe\n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor soluci√≥n que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique())} | study.best_params\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model2 = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "print(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))\n",
    "\n",
    "display(plot_confusion_matrix(y_test,lgb_model2.predict(X_test).argmax(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52853014",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros con 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(), dy_pred.argmax(axis=1), weights='quadratic')\n",
    "    is_higher_better = True\n",
    "    return (metric_name, value, is_higher_better)\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    \n",
    "    # Inicio el store de artefactos (archivos) de Optuna\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Hiperpar√°metros a optimizar\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    scores_ensemble = np.zeros((len(y_test), len(y_train.unique())))\n",
    "    score_folds = 0\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=False)\n",
    "        \n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=False)\n",
    "                \n",
    "        lgb_model3 = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_if_dataset,\n",
    "            valid_sets=lgb_oof_dataset,\n",
    "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "            feval=lgb_custom_metric_kappa\n",
    "        )\n",
    "        \n",
    "        scores_ensemble += lgb_model3.predict(X_test)\n",
    "\n",
    "        score_folds += cohen_kappa_score(\n",
    "            y_train.iloc[oof_index],\n",
    "            lgb_model3.predict(X_train.iloc[oof_index]).argmax(axis=1),\n",
    "            weights='quadratic'\n",
    "        ) / n_splits\n",
    "    \n",
    "    # Guardar predicciones sobre test\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    predicted_df = test.copy()\n",
    "    predicted_df['pred'] = [scores_ensemble[p, :] for p in range(scores_ensemble.shape[0])]\n",
    "    dump(predicted_df, predicted_filename)\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar las predicciones como artefacto.\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Generar nombre de archivo (¬°esto es un string, no lo pises!)\n",
    "    cm_filename = os.path.join(PATH_TO_TEMP_FILES, f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "\n",
    "    # Crear y guardar matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, scores_ensemble.argmax(axis=1))\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot()\n",
    "    plt.savefig(cm_filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar la imagen como artefacto\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=cm_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "    \n",
    "    # Guardar el score en test como m√©trica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, scores_ensemble.argmax(axis=1), weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    # Devuelvo el promedio de los scores del CV (objetivo a maximizar)\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_JM_30.sqlite3\",\n",
    "                            study_name=\"100_LGBM_FE_JM_30_CV\",\n",
    "                            load_if_exists = True)\n",
    "\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a5d3b",
   "metadata": {},
   "source": [
    "## LGBM Importance Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "features = np.array(features)\n",
    "model= lgb_model2\n",
    "\n",
    "# --------------------\n",
    "# 1. Feature Importance basada en GAIN\n",
    "# --------------------\n",
    "importances_gain = model.feature_importance(importance_type='gain')\n",
    "sorted_idx_gain = importances_gain.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_gain], importances_gain[sorted_idx_gain])\n",
    "plt.xlabel('Importancia (GAIN)')\n",
    "plt.title('Feature Importance basada en GAIN')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 2. Feature Importance basada en SPLIT\n",
    "# --------------------\n",
    "importances_split = model.feature_importance(importance_type='split')\n",
    "sorted_idx_split = importances_split.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_split], importances_split[sorted_idx_split])\n",
    "plt.xlabel('Importancia (SPLIT)')\n",
    "plt.title('Feature Importance basada en SPLIT')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 3. Permutation Importance\n",
    "# --------------------\n",
    "# Para permutation necesitamos un modelo tipo sklearn, armamos uno r√°pido\n",
    "model_sklearn = lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "perm_result = permutation_importance(model_sklearn, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "sorted_idx_perm = perm_result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_perm], perm_result.importances_mean[sorted_idx_perm])\n",
    "plt.xlabel('Impacto en Accuracy')\n",
    "plt.title('Permutation Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 4. SHAP Values\n",
    "# --------------------\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Graficar importancia promedio\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features, plot_type=\"dot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
