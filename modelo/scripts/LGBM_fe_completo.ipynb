{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062ae9ab",
   "metadata": {},
   "source": [
    "## Transformaciones implementadas (Feature Engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f3197",
   "metadata": {},
   "source": [
    "### Descripcion General\n",
    "\n",
    "Se generaron variables en base a las conclusiones obtenidas en el an√°lisis EDA con el fin de mejorar el rendimiento del modelo de forma intuitiva, considerando que podr√≠a influir en su velocidad de adopci√≥n. Luego se procedio a evaluar las variables en base a m√©tricas del modelo LGBM (Model-based Feature Evaluation).\n",
    "\n",
    "Se utilizaron m√©tricas internas del modelo (como GAIN, SPLIT en LGBM) para evaluar y seleccionar o mejorar variables.\n",
    "\n",
    "Se fueron eliminando y/o agregando otras variables en base al an√°lisis del apartado \"LGBM Futures Importance\" que despliega una serie de gr√°ficos que muestran las variables m√°s importantes en el modelado, en base a su influencia en los siguientes par√°metros: GAIN, SPLIT, Permutation y SHAP Values (relaciones no lineales entre variables).\n",
    "\n",
    "Este an√°lisis combinado con el EDA permiti√≥ generar un conjunto de variables mas eficientes para el modelado del procesamiento tabular.\n",
    "\n",
    "**Estas son las variables que finalmente quedaron:**\n",
    "\n",
    "HasName : Indica si el animal tiene nombre.\n",
    "\n",
    "PureBreed: Indica si el animal es de raza pura (Breed2 == 0 y no contiene \"Mixed.\n",
    "\n",
    "DescLength: Longitud de la descripci√≥n.\n",
    "\n",
    "Fee_log: Logaritmo del precio para suavizar la distribuci√≥n y reducir el impacto de valores extremos.\n",
    "\n",
    "IsFree: Si la adopci√≥n es gratis.\n",
    "\n",
    "Type_Breed_Combo: Combina el tipo de animal con el ID de la raza principal.\n",
    "\n",
    "Fee_Breed_Ratio: Relaci√≥n entre la tarifa individual y el promedio de tarifa para su raza.\n",
    "\n",
    "Fee_per_Pet: Tarifa dividida por cantidad de mascotas.\n",
    "\n",
    "Photo_per_Pet: Cantidad de fotos por mascota.\n",
    "\n",
    "Age_Fee_Ratio: Edad dividida por tarifa m√°s 1 (edad vs. valor monetario).\n",
    "\n",
    "DescLength_per_Pet: Longitud de descripci√≥n dividida por cantidad de mascotas.\n",
    "\n",
    "DescLength_per_Photo: Longitud de descripci√≥n dividida por cantidad de fotos.\n",
    "\n",
    "AgeGroup: Categorias de edades.\n",
    "\n",
    "### An√°lisis de Variables\n",
    "\n",
    "#### Objetivo del an√°lisis\n",
    "\n",
    "El prop√≥sito de esta secci√≥n es examinar detalladamente cada una de las variables presentes o construidas a partir del dataset original, evaluando su comportamiento, transformaciones aplicadas y relevancia potencial para predecir la variable objetivo `AdoptionSpeed`. Posteriormente, se comparan estos hallazgos con los resultados obtenidos del modelo LGBM a trav√©s de t√©cnicas de evaluaci√≥n de importancia de variables (GAIN, SPLIT, Permutation y SHAP).\n",
    "\n",
    "#### Variables analizadas y conclusiones\n",
    "\n",
    "A continuaci√≥n, se detallan las variables trabajadas y las conclusiones alcanzadas a partir del an√°lisis exploratorio de datos (EDA):\n",
    "\n",
    "üîπ Edad (`Age`, `AgeGroup`, `AgeBins`, `Age_Type`)\n",
    "\n",
    "La edad es la variable m√°s influyente. Los animales j√≥venes (especialmente menores de 6 meses) son adoptados con mayor rapidez. Se agruparon en categor√≠as (`AgeGroup`) para facilitar la interpretaci√≥n, y se combinaron con el tipo de animal (`Age_Type`) para capturar interacciones significativas.\n",
    "\n",
    "üîπ Descripci√≥n (`DescLength`, `DescLength_per_Photo`, `DescLength_per_Pet`)\n",
    "\n",
    "Las descripciones largas y bien proporcionadas (en relaci√≥n a la cantidad de fotos o mascotas) tienen un impacto positivo en la adopci√≥n. Representan el esfuerzo del rescatista y la calidad de la publicaci√≥n.\n",
    "\n",
    "üîπ Tarifa de adopci√≥n (`Fee`, `Fee_gratis`, `Fee_paga`, `Fee_Breed_Ratio`)\n",
    "\n",
    "Si bien el efecto no es lineal, se observa que las mascotas con adopci√≥n gratuita suelen tener mayor velocidad de adopci√≥n. La relaci√≥n entre el precio y el promedio de su raza (`Fee_Breed_Ratio`) mejora el modelado de valores inusuales.\n",
    "\n",
    "üîπ Fotograf√≠as (`PhotoAmt`, `Photo_per_Pet`)\n",
    "\n",
    "El n√∫mero de fotos es un factor importante: m√°s im√°genes tienden a asociarse con mayor adoptabilidad. La variable `Photo_per_Pet` ajusta esta relaci√≥n en publicaciones de adopciones m√∫ltiples.\n",
    "\n",
    "üîπ Raza (`Breed1`, `Breed2`, `MixedBreed`, `BreedName1`, `BreedName2`)\n",
    "\n",
    "Algunas razas muestran tasas de adopci√≥n m√°s r√°pidas. Se identific√≥ si es raza mixta o pura (`MixedBreed`) y se incorporaron nombres para combinar con tipo. La combinaci√≥n `Type_Breed_Combo` fue muy importante en el modelo.\n",
    "\n",
    "üîπ Tipo y g√©nero (`Type`, `Gender`, `Quantity`)\n",
    "\n",
    "Perros suelen ser adoptados m√°s r√°pido que gatos. El g√©nero tiene un efecto leve pero relevante en interacci√≥n con la edad. La cantidad de animales por publicaci√≥n puede diluir la atenci√≥n por cada uno.\n",
    "\n",
    "üîπ Caracter√≠sticas f√≠sicas y sanitarias (`MaturitySize`, `FurLength`, `Health`)\n",
    "\n",
    "Tama√±os medianos y pelajes cortos tienden a facilitar la adopci√≥n. Condiciones sanitarias como `Health`, `Sterilized`, `Vaccinated` o `Dewormed` reflejan confianza y cuidado, aunque su impacto directo fue m√°s limitado.\n",
    "\n",
    "üîπ Coloraci√≥n (`Color1`, `Color2`, `Color3`, `color_n`, `color_tipo`)\n",
    "\n",
    "Se analizaron combinaciones y tipos de color, pero no mostraron una relaci√≥n s√≥lida con la velocidad de adopci√≥n.\n",
    "\n",
    "El modelo LGBM, entrenado con las variables trabajadas, arroj√≥ los siguientes resultados consistentes:\n",
    "\n",
    "- Las variables m√°s relevantes fueron:\n",
    "\n",
    " `Age`, `DescLength_per_Photo`, `DescLength`, `DescLength_per_Pet`, `Type_Breed_Combo`, `Age_Fee_Ratio`, `Photo_per_Pet`\n",
    "\n",
    "- Variables con bajo impacto seg√∫n Permutation Importance:\n",
    "\n",
    " `VideoAmt`, `HasName`, `Health`, `Color3`, `PureBreed`, `AgeGroup`\n",
    "\n",
    "- SHAP revel√≥ interacciones importantes especialmente entre `Age` y `Gender`, `Breed1`, y `Breed2`, lo que justifica la exploraci√≥n de combinaciones como `Age_Type` o segmentaciones cruzadas.\n",
    "\n",
    "### Conclusi√≥n Final\n",
    "\n",
    "El an√°lisis exploratorio confirm√≥ que las variables m√°s importantes para predecir la velocidad de adopci√≥n son la edad, el esfuerzo de presentaci√≥n (descripci√≥n y fotos), la combinaci√≥n tipo-raza, y la relaci√≥n entre tarifa y edad. El modelo LGBM valid√≥ estas conclusiones, resaltando la utilidad de las variables derivadas y combinadas sobre las originales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c37b40",
   "metadata": {},
   "source": [
    "## Configuracion de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3edd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si est√°s en Jupyter Notebook, ancl√°s a una ruta conocida o fija:\n",
    "project_root = r\"C:\\Users\\juanm\\GitHub\\UA_MDM_Labo2_G9\"  # Personalizar esto\n",
    "\n",
    "# Alternativa: usar una carpeta conocida dentro del proyecto\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Menos fr√°gil\n",
    "\n",
    "# Rutas de trabajo\n",
    "workDir = os.path.join(project_root, \"work\")\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(optunaArtifactDir, exist_ok=True)\n",
    "os.makedirs(optunaTempDir, exist_ok=True)\n",
    "\n",
    "# Cambiar al directorio de trabajo solo si es necesario\n",
    "if os.getcwd() != workDir:\n",
    "    os.chdir(workDir)\n",
    "\n",
    "print(\"Directorio actual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulaci√≥n de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Modelado: Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Sklearn: splits, m√©tricas y utilidades\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt  # Usar junto con seaborn o plotly si es necesario\n",
    "import plotly.express as px      # Correcci√≥n: no usar `from plotly import express as px`\n",
    "\n",
    "# Optuna para optimizaci√≥n de hiperpar√°metros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "sampler = TPESampler(seed=SEED)  # fijo la semilla global de exploraci√≥n\n",
    "TEST_SIZE = 0.2 #Facci√≥n para train/test= split\n",
    "\n",
    "# Utilidades propias\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "# Guardado de objetos\n",
    "from joblib import dump, load\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b908605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths para acceso archivos\n",
    "#Este notebook asume la siguiente estructura de carpetas a partir de la ubicacion de base_dir \n",
    "#(dos niveles arriba de la c∆íarpeta donde se ejecuta el notebook). \n",
    "# /∆í/∆í\n",
    "# /UA_MDM_Labo2/input∆í\n",
    "# /UA_MDM_Labo2/input/petfinder-adoption-prediction/            <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# /UA_MDM_Labo2/tutoriales/                       <- Aca deben poner los notebooks y scripts que les compartimos\n",
    "# /UA_MDM_Labo2/work/                             <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# /UA_MDM_Labo2/work/models/                     <- Modelos entrenados en archivos joblibs\n",
    "# /UA_MDM_Labo2/work/optuna_temp_artifacts/      <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# /UA_MDM_Labo2/work/optuna_artifacts/           <- Archivos con artefactos que sibimos a optuna\n",
    "\n",
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Datos de razas \n",
    "PATH_TO_BREED_LABELS = os.path.join(BASE_DIR, \"modelo/data/petfinder-adoption-prediction/BreedLabels.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e967956",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd789d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# 1. Tiene nombre\n",
    "dataset['HasName'] = dataset['Name'].notnull().astype(int)\n",
    "\n",
    "# 2. Es de raza pura\n",
    "# Cargar etiquetas de razas\n",
    "breed_labels = pd.read_csv(PATH_TO_BREED_LABELS)\n",
    "\n",
    "# Diccionario ID ‚Üí Nombre de raza\n",
    "id_to_breed = dict(zip(breed_labels['BreedID'], breed_labels['BreedName']))\n",
    "\n",
    "# Crear una nueva columna con el nombre de Breed1\n",
    "dataset['Breed1_name'] = dataset['Breed1'].map(id_to_breed)\n",
    "\n",
    "dataset['PureBreed'] = (\n",
    "    (dataset['Breed2'] == 0) &\n",
    "    (~dataset['Breed1_name'].str.contains('Mixed', case=False, na=False))\n",
    ").astype(int)\n",
    "\n",
    "# 3. Longitud de la descripci√≥n\n",
    "dataset['DescLength'] = dataset['Description'].fillna('').apply(len)\n",
    "\n",
    "# 4. Combinaci√≥n tipo-raza principal\n",
    "dataset['Type_Breed_Combo'] = dataset['Type'].astype(str) + '_' + dataset['Breed1'].astype(str)\n",
    "\n",
    "dataset['Type_Breed_Combo'] = dataset['Type_Breed_Combo'].astype('category')\n",
    "\n",
    "# 5. Tarifa por mascota\n",
    "dataset['Fee_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['Fee'] / dataset['Quantity'])\n",
    "\n",
    "# 6. Fotos por mascota\n",
    "dataset['Photo_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['PhotoAmt'] / dataset['Quantity'])\n",
    "\n",
    "# 7. Relaci√≥n edad/tarifa\n",
    "dataset['Age_Fee_Ratio'] = dataset['Age'] / (dataset['Fee'] + 1)\n",
    "\n",
    "# 8. Longitud descripci√≥n por mascota\n",
    "dataset['DescLength_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['DescLength'] / dataset['Quantity'])\n",
    "\n",
    "# 9. Longitud descripci√≥n por cantidad de fotos \n",
    "dataset['DescLength_per_Photo'] = np.where(dataset['PhotoAmt'] == 0,0,dataset['DescLength'] / dataset['PhotoAmt'])\n",
    "\n",
    "# 10. Tarifa promedio por raza\n",
    "breed_fee_mean = dataset.groupby('Breed1')['Fee'].mean()\n",
    "\n",
    "dataset['Fee_Breed_Ratio'] = dataset['Fee'] / (dataset['Breed1'].map(breed_fee_mean) + 1)\n",
    "\n",
    "# 11. Categorias de edades\n",
    "def categorizar_edad(meses):\n",
    "    if pd.isna(meses):\n",
    "        return np.nan\n",
    "    elif meses <= 6:\n",
    "        return 'Cachorro'\n",
    "    elif meses <= 24:\n",
    "        return 'Joven'\n",
    "    elif meses <= 72:\n",
    "        return 'Adulto'\n",
    "    elif meses <= 120:\n",
    "        return 'Mayor'\n",
    "    else:\n",
    "        return 'Anciano'\n",
    "\n",
    "dataset['AgeGroup'] = dataset['Age'].apply(categorizar_edad)\n",
    "dataset['AgeGroup'] = pd.Categorical(dataset['AgeGroup'], categories=['Cachorro', 'Joven', 'Adulto', 'Mayor', 'Anciano'], ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)\n",
    "\n",
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']\n",
    "\n",
    "#Defino features a usar\n",
    "features = numeric_feats.copy()\n",
    "\n",
    "label = 'AdoptionSpeed'\n",
    "\n",
    "# Eliminamos 'AdoptionSpeed' si est√°\n",
    "if 'AdoptionSpeed' in features:\n",
    "    features.remove('AdoptionSpeed')\n",
    "\n",
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09eae2",
   "metadata": {},
   "source": [
    "## Modelo con hiperparametros por default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cacb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoi√≥n\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        'seed':SEED\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model1 = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model1.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0683698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model1.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "print(cohen_kappa_score(y_test,y_pred, weights = 'quadratic'))\n",
    "\n",
    "#Muestro la matriz de confusi√≥n\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270bbba",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def lgb_objective(trial):\n",
    "    # Hiperpar√°metros\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'seed':SEED\n",
    "    }\n",
    "\n",
    "    # Dataset LightGBM\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train, label=y_train)\n",
    "\n",
    "    # Entrenamiento\n",
    "    lgb_model2 = lgb.train(lgb_params, lgb_train_dataset)\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred_proba = lgb_model2.predict(X_test)\n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    # Inicializar el store de artefactos\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Guardar predicciones\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    pred_df = test.copy()\n",
    "    pred_df['pred'] = [y_pred_proba[p, :] for p in range(y_pred_proba.shape[0])]\n",
    "    dump(pred_df, predicted_filename)\n",
    "\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Guardar m√©trica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    return test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_Completo.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_FE_Completo\", #nombre del experimento\n",
    "                            load_if_exists=True, #continuar si ya existe\n",
    "                            sampler=sampler) \n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor soluci√≥n que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        'seed':SEED} | study.best_params\n",
    "                        \n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model2 = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "print(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))\n",
    "\n",
    "display(plot_confusion_matrix(y_test,lgb_model2.predict(X_test).argmax(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52853014",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros con 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(), dy_pred.argmax(axis=1), weights='quadratic')\n",
    "    is_higher_better = True\n",
    "    return (metric_name, value, is_higher_better)\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    \n",
    "    # Inicio el store de artefactos (archivos) de Optuna\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Hiperpar√°metros a optimizar\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'n_jobs': -1,\n",
    "        'seed':SEED\n",
    "    }\n",
    "    \n",
    "    scores_ensemble = np.zeros((len(y_test), len(y_train.unique())))\n",
    "    score_folds = 0\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=False)\n",
    "        \n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=False)\n",
    "                \n",
    "        lgb_model3 = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_if_dataset,\n",
    "            valid_sets=lgb_oof_dataset,\n",
    "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "            feval=lgb_custom_metric_kappa\n",
    "        )\n",
    "        \n",
    "        scores_ensemble += lgb_model3.predict(X_test)\n",
    "\n",
    "        score_folds += cohen_kappa_score(\n",
    "            y_train.iloc[oof_index],\n",
    "            lgb_model3.predict(X_train.iloc[oof_index]).argmax(axis=1),\n",
    "            weights='quadratic'\n",
    "        ) / n_splits\n",
    "    \n",
    "    # Guardar predicciones sobre test\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    predicted_df = test.copy()\n",
    "    predicted_df['pred'] = [scores_ensemble[p, :] for p in range(scores_ensemble.shape[0])]\n",
    "    dump(predicted_df, predicted_filename)\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar las predicciones como artefacto.\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Guardar el score en test como m√©trica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, scores_ensemble.argmax(axis=1), weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    # Devuelvo el promedio de los scores del CV (objetivo a maximizar)\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_Completo.sqlite3\",\n",
    "                            study_name=\"100_LGBM_FE_Completo_CV\",\n",
    "                            load_if_exists = True,\n",
    "                            sampler=sampler)\n",
    "\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a5d3b",
   "metadata": {},
   "source": [
    "## LGBM Importance Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "features = np.array(features)\n",
    "model= lgb_model2\n",
    "\n",
    "# --------------------\n",
    "# 1. Feature Importance basada en GAIN\n",
    "# --------------------\n",
    "importances_gain = model.feature_importance(importance_type='gain')\n",
    "sorted_idx_gain = importances_gain.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_gain], importances_gain[sorted_idx_gain])\n",
    "plt.xlabel('Importancia (GAIN)')\n",
    "plt.title('Feature Importance basada en GAIN')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 2. Feature Importance basada en SPLIT\n",
    "# --------------------\n",
    "importances_split = model.feature_importance(importance_type='split')\n",
    "sorted_idx_split = importances_split.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_split], importances_split[sorted_idx_split])\n",
    "plt.xlabel('Importancia (SPLIT)')\n",
    "plt.title('Feature Importance basada en SPLIT')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 3. Permutation Importance\n",
    "# --------------------\n",
    "# Para permutation necesitamos un modelo tipo sklearn, armamos uno r√°pido\n",
    "model_sklearn = lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "perm_result = permutation_importance(model_sklearn, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "sorted_idx_perm = perm_result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_perm], perm_result.importances_mean[sorted_idx_perm])\n",
    "plt.xlabel('Impacto en Accuracy')\n",
    "plt.title('Permutation Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 4. SHAP Values\n",
    "# --------------------\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Graficar importancia promedio\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features, plot_type=\"dot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
