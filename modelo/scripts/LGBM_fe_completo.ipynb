{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062ae9ab",
   "metadata": {},
   "source": [
    "## Transformaciones implementadas (Feature Engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f3197",
   "metadata": {},
   "source": [
    "### Descripcion General\n",
    "\n",
    "Se generaron variables en base a las conclusiones obtenidas en el análisis EDA con el fin de mejorar el rendimiento del modelo de forma intuitiva, considerando que podría influir en su velocidad de adopción. Luego se procedio a evaluar las variables en base a métricas del modelo LGBM (Model-based Feature Evaluation).\n",
    "\n",
    "Se utilizaron métricas internas del modelo (como GAIN, SPLIT en LGBM) para evaluar y seleccionar o mejorar variables.\n",
    "\n",
    "Se fueron eliminando y/o agregando otras variables en base al análisis del apartado \"LGBM Futures Importance\" que despliega una serie de gráficos que muestran las variables más importantes en el modelado, en base a su influencia en los siguientes parámetros: GAIN, SPLIT, Permutation y SHAP Values (relaciones no lineales entre variables).\n",
    "\n",
    "Este análisis combinado con el EDA permitió generar un conjunto de variables mas eficientes para el modelado del procesamiento tabular.\n",
    "\n",
    "**Estas son las variables que finalmente quedaron:**\n",
    "\n",
    "HasName : Indica si el animal tiene nombre.\n",
    "\n",
    "PureBreed: Indica si el animal es de raza pura (Breed2 == 0 y no contiene \"Mixed.\n",
    "\n",
    "DescLength: Longitud de la descripción.\n",
    "\n",
    "Fee_log: Logaritmo del precio para suavizar la distribución y reducir el impacto de valores extremos.\n",
    "\n",
    "IsFree: Si la adopción es gratis.\n",
    "\n",
    "Type_Breed_Combo: Combina el tipo de animal con el ID de la raza principal.\n",
    "\n",
    "Fee_Breed_Ratio: Relación entre la tarifa individual y el promedio de tarifa para su raza.\n",
    "\n",
    "Fee_per_Pet: Tarifa dividida por cantidad de mascotas.\n",
    "\n",
    "Photo_per_Pet: Cantidad de fotos por mascota.\n",
    "\n",
    "Age_Fee_Ratio: Edad dividida por tarifa más 1 (edad vs. valor monetario).\n",
    "\n",
    "DescLength_per_Pet: Longitud de descripción dividida por cantidad de mascotas.\n",
    "\n",
    "DescLength_per_Photo: Longitud de descripción dividida por cantidad de fotos.\n",
    "\n",
    "AgeGroup: Categorias de edades.\n",
    "\n",
    "### Análisis de Variables\n",
    "\n",
    "#### Objetivo del análisis\n",
    "\n",
    "El propósito de esta sección es examinar detalladamente cada una de las variables presentes o construidas a partir del dataset original, evaluando su comportamiento, transformaciones aplicadas y relevancia potencial para predecir la variable objetivo `AdoptionSpeed`. Posteriormente, se comparan estos hallazgos con los resultados obtenidos del modelo LGBM a través de técnicas de evaluación de importancia de variables (GAIN, SPLIT, Permutation y SHAP).\n",
    "\n",
    "#### Variables analizadas y conclusiones\n",
    "\n",
    "A continuación, se detallan las variables trabajadas y las conclusiones alcanzadas a partir del análisis exploratorio de datos (EDA):\n",
    "\n",
    "🔹 Edad (`Age`, `AgeGroup`, `AgeBins`, `Age_Type`)\n",
    "\n",
    "La edad es la variable más influyente. Los animales jóvenes (especialmente menores de 6 meses) son adoptados con mayor rapidez. Se agruparon en categorías (`AgeGroup`) para facilitar la interpretación, y se combinaron con el tipo de animal (`Age_Type`) para capturar interacciones significativas.\n",
    "\n",
    "🔹 Descripción (`DescLength`, `DescLength_per_Photo`, `DescLength_per_Pet`)\n",
    "\n",
    "Las descripciones largas y bien proporcionadas (en relación a la cantidad de fotos o mascotas) tienen un impacto positivo en la adopción. Representan el esfuerzo del rescatista y la calidad de la publicación.\n",
    "\n",
    "🔹 Tarifa de adopción (`Fee`, `Fee_gratis`, `Fee_paga`, `Fee_Breed_Ratio`)\n",
    "\n",
    "Si bien el efecto no es lineal, se observa que las mascotas con adopción gratuita suelen tener mayor velocidad de adopción. La relación entre el precio y el promedio de su raza (`Fee_Breed_Ratio`) mejora el modelado de valores inusuales.\n",
    "\n",
    "🔹 Fotografías (`PhotoAmt`, `Photo_per_Pet`)\n",
    "\n",
    "El número de fotos es un factor importante: más imágenes tienden a asociarse con mayor adoptabilidad. La variable `Photo_per_Pet` ajusta esta relación en publicaciones de adopciones múltiples.\n",
    "\n",
    "🔹 Raza (`Breed1`, `Breed2`, `MixedBreed`, `BreedName1`, `BreedName2`)\n",
    "\n",
    "Algunas razas muestran tasas de adopción más rápidas. Se identificó si es raza mixta o pura (`MixedBreed`) y se incorporaron nombres para combinar con tipo. La combinación `Type_Breed_Combo` fue muy importante en el modelo.\n",
    "\n",
    "🔹 Tipo y género (`Type`, `Gender`, `Quantity`)\n",
    "\n",
    "Perros suelen ser adoptados más rápido que gatos. El género tiene un efecto leve pero relevante en interacción con la edad. La cantidad de animales por publicación puede diluir la atención por cada uno.\n",
    "\n",
    "🔹 Características físicas y sanitarias (`MaturitySize`, `FurLength`, `Health`)\n",
    "\n",
    "Tamaños medianos y pelajes cortos tienden a facilitar la adopción. Condiciones sanitarias como `Health`, `Sterilized`, `Vaccinated` o `Dewormed` reflejan confianza y cuidado, aunque su impacto directo fue más limitado.\n",
    "\n",
    "🔹 Coloración (`Color1`, `Color2`, `Color3`, `color_n`, `color_tipo`)\n",
    "\n",
    "Se analizaron combinaciones y tipos de color, pero no mostraron una relación sólida con la velocidad de adopción.\n",
    "\n",
    "El modelo LGBM, entrenado con las variables trabajadas, arrojó los siguientes resultados consistentes:\n",
    "\n",
    "- Las variables más relevantes fueron:\n",
    "\n",
    " `Age`, `DescLength_per_Photo`, `DescLength`, `DescLength_per_Pet`, `Type_Breed_Combo`, `Age_Fee_Ratio`, `Photo_per_Pet`\n",
    "\n",
    "- Variables con bajo impacto según Permutation Importance:\n",
    "\n",
    " `VideoAmt`, `HasName`, `Health`, `Color3`, `PureBreed`, `AgeGroup`\n",
    "\n",
    "- SHAP reveló interacciones importantes especialmente entre `Age` y `Gender`, `Breed1`, y `Breed2`, lo que justifica la exploración de combinaciones como `Age_Type` o segmentaciones cruzadas.\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "El análisis exploratorio confirmó que las variables más importantes para predecir la velocidad de adopción son la edad, el esfuerzo de presentación (descripción y fotos), la combinación tipo-raza, y la relación entre tarifa y edad. El modelo LGBM validó estas conclusiones, resaltando la utilidad de las variables derivadas y combinadas sobre las originales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c37b40",
   "metadata": {},
   "source": [
    "## Configuracion de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3edd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si estás en Jupyter Notebook, anclás a una ruta conocida o fija:\n",
    "project_root = r\"C:\\Users\\juanm\\GitHub\\UA_MDM_Labo2_G9\"  # Personalizar esto\n",
    "\n",
    "# Alternativa: usar una carpeta conocida dentro del proyecto\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Menos frágil\n",
    "\n",
    "# Rutas de trabajo\n",
    "workDir = os.path.join(project_root, \"work\")\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(optunaArtifactDir, exist_ok=True)\n",
    "os.makedirs(optunaTempDir, exist_ok=True)\n",
    "\n",
    "# Cambiar al directorio de trabajo solo si es necesario\n",
    "if os.getcwd() != workDir:\n",
    "    os.chdir(workDir)\n",
    "\n",
    "print(\"Directorio actual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Modelado: Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Sklearn: splits, métricas y utilidades\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt  # Usar junto con seaborn o plotly si es necesario\n",
    "import plotly.express as px      # Corrección: no usar `from plotly import express as px`\n",
    "\n",
    "# Optuna para optimización de hiperparámetros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "sampler = TPESampler(seed=SEED)  # fijo la semilla global de exploración\n",
    "TEST_SIZE = 0.2 #Facción para train/test= split\n",
    "\n",
    "# Utilidades propias\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "# Guardado de objetos\n",
    "from joblib import dump, load\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b908605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths para acceso archivos\n",
    "#Este notebook asume la siguiente estructura de carpetas a partir de la ubicacion de base_dir \n",
    "#(dos niveles arriba de la cƒarpeta donde se ejecuta el notebook). \n",
    "# /ƒ/ƒ\n",
    "# /UA_MDM_Labo2/inputƒ\n",
    "# /UA_MDM_Labo2/input/petfinder-adoption-prediction/            <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# /UA_MDM_Labo2/tutoriales/                       <- Aca deben poner los notebooks y scripts que les compartimos\n",
    "# /UA_MDM_Labo2/work/                             <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# /UA_MDM_Labo2/work/models/                     <- Modelos entrenados en archivos joblibs\n",
    "# /UA_MDM_Labo2/work/optuna_temp_artifacts/      <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# /UA_MDM_Labo2/work/optuna_artifacts/           <- Archivos con artefactos que sibimos a optuna\n",
    "\n",
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Datos de razas \n",
    "PATH_TO_BREED_LABELS = os.path.join(BASE_DIR, \"modelo/data/petfinder-adoption-prediction/BreedLabels.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e967956",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd789d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# 1. Tiene nombre\n",
    "dataset['HasName'] = dataset['Name'].notnull().astype(int)\n",
    "\n",
    "# 2. Es de raza pura\n",
    "# Cargar etiquetas de razas\n",
    "breed_labels = pd.read_csv(PATH_TO_BREED_LABELS)\n",
    "\n",
    "# Diccionario ID → Nombre de raza\n",
    "id_to_breed = dict(zip(breed_labels['BreedID'], breed_labels['BreedName']))\n",
    "\n",
    "# Crear una nueva columna con el nombre de Breed1\n",
    "dataset['Breed1_name'] = dataset['Breed1'].map(id_to_breed)\n",
    "\n",
    "dataset['PureBreed'] = (\n",
    "    (dataset['Breed2'] == 0) &\n",
    "    (~dataset['Breed1_name'].str.contains('Mixed', case=False, na=False))\n",
    ").astype(int)\n",
    "\n",
    "# 3. Longitud de la descripción\n",
    "dataset['DescLength'] = dataset['Description'].fillna('').apply(len)\n",
    "\n",
    "# 4. Combinación tipo-raza principal\n",
    "dataset['Type_Breed_Combo'] = dataset['Type'].astype(str) + '_' + dataset['Breed1'].astype(str)\n",
    "\n",
    "dataset['Type_Breed_Combo'] = dataset['Type_Breed_Combo'].astype('category')\n",
    "\n",
    "# 5. Tarifa por mascota\n",
    "dataset['Fee_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['Fee'] / dataset['Quantity'])\n",
    "\n",
    "# 6. Fotos por mascota\n",
    "dataset['Photo_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['PhotoAmt'] / dataset['Quantity'])\n",
    "\n",
    "# 7. Relación edad/tarifa\n",
    "dataset['Age_Fee_Ratio'] = dataset['Age'] / (dataset['Fee'] + 1)\n",
    "\n",
    "# 8. Longitud descripción por mascota\n",
    "dataset['DescLength_per_Pet'] = np.where(dataset['Quantity'] == 0, 0, dataset['DescLength'] / dataset['Quantity'])\n",
    "\n",
    "# 9. Longitud descripción por cantidad de fotos \n",
    "dataset['DescLength_per_Photo'] = np.where(dataset['PhotoAmt'] == 0,0,dataset['DescLength'] / dataset['PhotoAmt'])\n",
    "\n",
    "# 10. Tarifa promedio por raza\n",
    "breed_fee_mean = dataset.groupby('Breed1')['Fee'].mean()\n",
    "\n",
    "dataset['Fee_Breed_Ratio'] = dataset['Fee'] / (dataset['Breed1'].map(breed_fee_mean) + 1)\n",
    "\n",
    "# 11. Categorias de edades\n",
    "def categorizar_edad(meses):\n",
    "    if pd.isna(meses):\n",
    "        return np.nan\n",
    "    elif meses <= 6:\n",
    "        return 'Cachorro'\n",
    "    elif meses <= 24:\n",
    "        return 'Joven'\n",
    "    elif meses <= 72:\n",
    "        return 'Adulto'\n",
    "    elif meses <= 120:\n",
    "        return 'Mayor'\n",
    "    else:\n",
    "        return 'Anciano'\n",
    "\n",
    "dataset['AgeGroup'] = dataset['Age'].apply(categorizar_edad)\n",
    "dataset['AgeGroup'] = pd.Categorical(dataset['AgeGroup'], categories=['Cachorro', 'Joven', 'Adulto', 'Mayor', 'Anciano'], ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)\n",
    "\n",
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']\n",
    "\n",
    "#Defino features a usar\n",
    "features = numeric_feats.copy()\n",
    "\n",
    "label = 'AdoptionSpeed'\n",
    "\n",
    "# Eliminamos 'AdoptionSpeed' si está\n",
    "if 'AdoptionSpeed' in features:\n",
    "    features.remove('AdoptionSpeed')\n",
    "\n",
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09eae2",
   "metadata": {},
   "source": [
    "## Modelo con hiperparametros por default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cacb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoión\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        'seed':SEED\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model1 = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model1.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0683698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model1.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "print(cohen_kappa_score(y_test,y_pred, weights = 'quadratic'))\n",
    "\n",
    "#Muestro la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270bbba",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def lgb_objective(trial):\n",
    "    # Hiperparámetros\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'seed':SEED\n",
    "    }\n",
    "\n",
    "    # Dataset LightGBM\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train, label=y_train)\n",
    "\n",
    "    # Entrenamiento\n",
    "    lgb_model2 = lgb.train(lgb_params, lgb_train_dataset)\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred_proba = lgb_model2.predict(X_test)\n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    # Inicializar el store de artefactos\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Guardar predicciones\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    pred_df = test.copy()\n",
    "    pred_df['pred'] = [y_pred_proba[p, :] for p in range(y_pred_proba.shape[0])]\n",
    "    dump(pred_df, predicted_filename)\n",
    "\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Guardar métrica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    return test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_Completo.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_FE_Completo\", #nombre del experimento\n",
    "                            load_if_exists=True, #continuar si ya existe\n",
    "                            sampler=sampler) \n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor solución que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        'seed':SEED} | study.best_params\n",
    "                        \n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model2 = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "print(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))\n",
    "\n",
    "display(plot_confusion_matrix(y_test,lgb_model2.predict(X_test).argmax(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52853014",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros con 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(), dy_pred.argmax(axis=1), weights='quadratic')\n",
    "    is_higher_better = True\n",
    "    return (metric_name, value, is_higher_better)\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    \n",
    "    # Inicio el store de artefactos (archivos) de Optuna\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Hiperparámetros a optimizar\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'n_jobs': -1,\n",
    "        'seed':SEED\n",
    "    }\n",
    "    \n",
    "    scores_ensemble = np.zeros((len(y_test), len(y_train.unique())))\n",
    "    score_folds = 0\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=False)\n",
    "        \n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=False)\n",
    "                \n",
    "        lgb_model3 = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_if_dataset,\n",
    "            valid_sets=lgb_oof_dataset,\n",
    "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "            feval=lgb_custom_metric_kappa\n",
    "        )\n",
    "        \n",
    "        scores_ensemble += lgb_model3.predict(X_test)\n",
    "\n",
    "        score_folds += cohen_kappa_score(\n",
    "            y_train.iloc[oof_index],\n",
    "            lgb_model3.predict(X_train.iloc[oof_index]).argmax(axis=1),\n",
    "            weights='quadratic'\n",
    "        ) / n_splits\n",
    "    \n",
    "    # Guardar predicciones sobre test\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    predicted_df = test.copy()\n",
    "    predicted_df['pred'] = [scores_ensemble[p, :] for p in range(scores_ensemble.shape[0])]\n",
    "    dump(predicted_df, predicted_filename)\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar las predicciones como artefacto.\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Guardar el score en test como métrica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, scores_ensemble.argmax(axis=1), weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    # Devuelvo el promedio de los scores del CV (objetivo a maximizar)\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_FE_Completo.sqlite3\",\n",
    "                            study_name=\"100_LGBM_FE_Completo_CV\",\n",
    "                            load_if_exists = True,\n",
    "                            sampler=sampler)\n",
    "\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a5d3b",
   "metadata": {},
   "source": [
    "## LGBM Importance Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "features = np.array(features)\n",
    "model= lgb_model2\n",
    "\n",
    "# --------------------\n",
    "# 1. Feature Importance basada en GAIN\n",
    "# --------------------\n",
    "importances_gain = model.feature_importance(importance_type='gain')\n",
    "sorted_idx_gain = importances_gain.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_gain], importances_gain[sorted_idx_gain])\n",
    "plt.xlabel('Importancia (GAIN)')\n",
    "plt.title('Feature Importance basada en GAIN')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 2. Feature Importance basada en SPLIT\n",
    "# --------------------\n",
    "importances_split = model.feature_importance(importance_type='split')\n",
    "sorted_idx_split = importances_split.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_split], importances_split[sorted_idx_split])\n",
    "plt.xlabel('Importancia (SPLIT)')\n",
    "plt.title('Feature Importance basada en SPLIT')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 3. Permutation Importance\n",
    "# --------------------\n",
    "# Para permutation necesitamos un modelo tipo sklearn, armamos uno rápido\n",
    "model_sklearn = lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "perm_result = permutation_importance(model_sklearn, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "sorted_idx_perm = perm_result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_perm], perm_result.importances_mean[sorted_idx_perm])\n",
    "plt.xlabel('Impacto en Accuracy')\n",
    "plt.title('Permutation Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 4. SHAP Values\n",
    "# --------------------\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Graficar importancia promedio\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features, plot_type=\"dot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
