{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo datos estructurados\n",
    "Este notebook desarrolla un primer modelo para resolver el problema de Petfinder. Empezamos haciendo un modelo inicial muy simple para ver la viabilidad de resolver el problema. Luego analizamos como se comporta la métrica kappa propuesta y vemos la matriz de confusión. Finalmente hacemos una optimizacin de hiperparametros evaluando con train/test y otra validando con 5 fold CV y testeando en el 20% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si estás en Jupyter Notebook, anclás a una ruta conocida o fija:\n",
    "project_root = r\"C:\\Users\\juanm\\GitHub\\UA_MDM_Labo2_G9\"  # Personalizar esto\n",
    "\n",
    "# Rutas de trabajo\n",
    "workDir = os.path.join(project_root, \"work\")\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(optunaArtifactDir, exist_ok=True)\n",
    "os.makedirs(optunaTempDir, exist_ok=True)\n",
    "\n",
    "# Cambiar al directorio de trabajo solo si es necesario\n",
    "if os.getcwd() != workDir:\n",
    "    os.chdir(workDir)\n",
    "\n",
    "print(\"Directorio actual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import de librerias basicas tablas y matrices\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Funciones auxiliares sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold #Split y cross Validation\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score #Metricas\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "#Visualizacióon\n",
    "from plotly import express as px\n",
    "\n",
    "#Plot de matriz de confusion normalizada en actuals\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "#Optimizacion de hiperparametros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "#Guardado de objetos en archivos joblib\n",
    "from joblib import load, dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths para acceso archivos\n",
    "\n",
    "# os.makedirs(\"../inputƒ\")\n",
    "# os.makedirs(\"../input/petfinder-adoption-prediction/\") #  <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# os.makedirs(\"../work/\")                                #  <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# os.makedirs(\"../work/models/\")                         #  <- Modelos entrenados en archivos joblibs\n",
    "# os.makedirs(\"../work/optuna_temp_artifacts/\")          #  <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# os.makedirs(\"../work/optuna_artifacts/\")               #  <- Archivos con artefactos que sibimos a optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Datos de razas \n",
    "PATH_TO_BREED_LABELS = os.path.join(BASE_DIR, \"modelo/data/petfinder-adoption-prediction/BreedLabels.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "TEST_SIZE = 0.2 #Facción para train/test= split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos Tabulares\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columnas del dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de features numericas\n",
    "numeric_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defino features a usar en un primer modelo de prueba\n",
    "features = ['Type',\n",
    " 'Age',\n",
    " 'Breed1',\n",
    " 'Breed2',\n",
    " 'Gender',\n",
    " 'Color1',\n",
    " 'Color2',\n",
    " 'Color3',\n",
    " 'MaturitySize',\n",
    " 'FurLength',\n",
    " 'Vaccinated',\n",
    " 'Dewormed',\n",
    " 'Sterilized',\n",
    " 'Health',\n",
    " 'Quantity',\n",
    " 'Fee',\n",
    " 'State',\n",
    " 'VideoAmt',\n",
    " 'PhotoAmt']\n",
    "\n",
    "label = 'AdoptionSpeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoión\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "cohen_kappa_score(y_test,y_pred, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestro la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a ponewr en perspectiva el score de Kappa\n",
    "\n",
    "\n",
    "#Cual es el score perfecto? Evaluo la clase real contra si misma. Es decir, el caso en que el modelo establece todas las clases en su valor real\n",
    "cohen_kappa_score(y_test,y_test, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como se veria la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a ver como se comporta kappa si hafgo una predicción al azar (respetando las proporciones de cada clase)\n",
    "y_shuffled = shuffle(y_test,\n",
    "                     random_state = 42)\n",
    "\n",
    "\n",
    "#Genero diccionarios para cambiar algunas predicicones reales por una prediccion cercana y_cerca y una lejana y_lejos a la real \n",
    "# ejemplo: la real se 0 voy a estimar 1 para la cercana y 4 para la lejana\n",
    "dict_map_cerca = {0:1,\n",
    "                  1:2,\n",
    "                  2:3,\n",
    "                  3:4,\n",
    "                  4:3}\n",
    "\n",
    "dict_map_lejos = {0:4,\n",
    "                  1:4,\n",
    "                  2:0,\n",
    "                  3:0,\n",
    "                  4:0}\n",
    "\n",
    "y_cerca = [dict_map_cerca[i] for i in y_test]\n",
    "\n",
    "y_lejos = [dict_map_lejos[i] for i in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Vamos a simular que la probabilidad de tener la prediccion real en casa muestra varia de 0 a 100. \n",
    "#Genero una numero aleatorio para cada muestra\n",
    "random_list =  np.random.rand(len(y_test))\n",
    "\n",
    "#inicializo un dataframe de resultados vacio\n",
    "kappa_progression = pd.DataFrame()\n",
    "\n",
    "#La variable i tiene un umbral para ir variando la cantidad de aciertos desde 0% a 100%\n",
    "for i in range(101):\n",
    "\n",
    "    #Genero la prediccion para i% de aciertos donde cuando no acierto me quedo con una prediccion al azar (podria ser la \"correcta\" pero solo por azar)\n",
    "    y_simulado = [y_test.iloc[sample] if random_list[sample]<i/100 else y_shuffled.iloc[sample] for sample in range(len(y_test))]\n",
    "\n",
    "    #Genero la prediccion para i% de aciertos donde cuando no acierto me quedo con una prediccion cercana o lejana a la correcta\n",
    "    y_simulado_cerca = [y_test.iloc[sample] if random_list[sample]<i/100 else y_cerca[sample] for sample in range(len(y_test))]\n",
    "    y_simulado_lejos = [y_test.iloc[sample] if random_list[sample]<i/100 else y_lejos[sample] for sample in range(len(y_test))]\n",
    "\n",
    "\n",
    "    #Grabo los resultados en un dataframe para cada i% de aciertos\n",
    "    kappa_progression = pd.concat([kappa_progression,\n",
    "                                   pd.DataFrame({'Conocidos':[i],\n",
    "                                                'kappa':cohen_kappa_score(y_test,\n",
    "                                                                        y_simulado,\n",
    "                                                                        weights = 'quadratic'),\n",
    "                                                'kappa_cerca':cohen_kappa_score(y_test,\n",
    "                                                                        y_simulado_cerca,\n",
    "                                                                        weights = 'quadratic'),\n",
    "                                                'kappa_lejos':cohen_kappa_score(y_test,\n",
    "                                                                        y_simulado_lejos,\n",
    "                                                                        weights = 'quadratic'),                                                                        \n",
    "                                                'accuracy':accuracy_score(y_test,\n",
    "                                                                        y_simulado),\n",
    "                                                'balanced_accuracy':balanced_accuracy_score(y_test,\n",
    "                                                                        y_simulado),\n",
    "                                                                        })],\n",
    "                ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico el comportamiento de la métrica a medida que incremento los aciertos. Tambien muestro lor resultados de otras metricas como Accuracy y Balanced Accuracy\n",
    "px.line(kappa_progression,x='Conocidos',y=['kappa',\n",
    "                                           'kappa_cerca',\n",
    "                                           'kappa_lejos',\n",
    "                                           'accuracy',\n",
    "                                           'balanced_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A modo de ejemplo muestro kappa y matriz de confusion para 50% de aciertos donde los errores quedan cerca de la clase correcta\n",
    "y_simulado_cerca = [y_test.iloc[sample] if random_list[sample]<50/100 else y_cerca[sample] for sample in range(len(y_test))]\n",
    "\n",
    "display(plot_confusion_matrix(y_test,y_simulado_cerca, \n",
    "                              title = \"Kappa \" + str(cohen_kappa_score(y_test,y_simulado_cerca, weights = 'quadratic'))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A modo de ejemplo muestro kappa y matriz de confusion para 50% de aciertos donde los errores quedan lejos de la clase correcta\n",
    "y_simulado_lejos = [y_test.iloc[sample] if random_list[sample]<50/100 else y_lejos[sample] for sample in range(len(y_test))]\n",
    "\n",
    "display(plot_confusion_matrix(y_test,y_simulado_lejos, \n",
    "                              title = \"Kappa \" + str(cohen_kappa_score(y_test,y_simulado_lejos, weights = 'quadratic'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruebo un modelo alternativo donde en vez de usar la version multiclass real de lightGBM utilizo One vs All\n",
    "\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclassova',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAtriz de confusion y Kappa dfe OVA\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "display(plot_confusion_matrix(y_test,y_pred))\n",
    "\n",
    "{'kappa':cohen_kappa_score(y_test,\n",
    "                y_pred,\n",
    "                weights = 'quadratic'),\n",
    " 'accuracy':accuracy_score(y_test,y_pred),\n",
    " 'balanced_accuracy':balanced_accuracy_score(y_test,y_pred)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizacion de hiperparametros modelo train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Funcion que vamos a optimizar. Optuna requiere que usemos el objeto trial para generar los parametros a optimizar\n",
    "def lgb_objective(trial):\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Genero objeto dataset de entrenamiento\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                    label=y_train)\n",
    "\n",
    "    #ajuste de modelo\n",
    "    lgb_model = lgb.train(lgb_params,\n",
    "                        lgb_train_dataset)\n",
    "    \n",
    "    #Devuelvo el score en test\n",
    "    return(cohen_kappa_score(y_test,lgb_model.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_Base_30.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_Base_30\", #nombre del experimento\n",
    "                            load_if_exists=True) #continuar si ya existe\n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor solución que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique())} | study.best_params\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "display(plot_confusion_matrix(y_test,lgb_model.predict(X_test).argmax(axis=1)))\n",
    "\n",
    "cohen_kappa_score(y_test,lgb_model.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con cross validation y conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero una metrica para que lightGBM haga la evaluación y pueda hacer early_stopping en el cross validation\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(),dy_pred.argmax(axis=1),weights = 'quadratic')\n",
    "    is_higher_better = True\n",
    "    return(metric_name, value, is_higher_better)\n",
    "\n",
    "#Funcion objetivo a optimizar. En este caso vamos a hacer 5fold cv sobre el conjunto de train. \n",
    "# El score de CV es el objetivo a optimizar. Ademas vamos a usar los 5 modelos del CV para estimar el conjunto de test,\n",
    "# registraremos en optuna las predicciones, matriz de confusion y el score en test.\n",
    "# CV Score -> Se usa para determinar el rendimiento de los hiperparametros con precision \n",
    "# Test Score -> Nos permite testear que esta todo OK, no use (ni debo usar) esos datos para nada en el entrenamiento \n",
    "# o la optimizacion de hiperparametros\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    \n",
    "    #Inicio el store de artefactos (archivos) de optuna\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Voy a generar estimaciones de los 5 modelos del CV sobre los datos test y los acumulo en la matriz scores_ensemble\n",
    "    scores_ensemble = np.zeros((len(y_test),len(y_train.unique())))\n",
    "\n",
    "    #Score del 5 fold CV inicializado en 0\n",
    "    score_folds = 0\n",
    "\n",
    "    #Numero de splits del CV\n",
    "    n_splits = 5\n",
    "\n",
    "    #Objeto para hacer el split estratificado de CV\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        \n",
    "        #Dataset in fold (donde entreno) \n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                        label=y_train.iloc[if_index],\n",
    "                                        free_raw_data=False)\n",
    "        \n",
    "        #Dataset Out of fold (donde mido la performance del CV)\n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                        label=y_train.iloc[oof_index],\n",
    "                                        free_raw_data=False)\n",
    "\n",
    "        #Entreno el modelo\n",
    "        lgb_model = lgb.train(lgb_params,\n",
    "                                lgb_if_dataset,\n",
    "                                valid_sets=lgb_oof_dataset,\n",
    "                                callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "                                feval = lgb_custom_metric_kappa\n",
    "                                )\n",
    "        \n",
    "        #Acumulo los scores (probabilidades) de cada clase para cada uno de los modelos que determino en los folds\n",
    "        #Se predice el 20% de los datos que separe para tes y no uso para entrenar en ningun fold\n",
    "        scores_ensemble = scores_ensemble + lgb_model.predict(X_test)\n",
    "        \n",
    "        #Score del fold (registros de dataset train que en este fold quedan out of fold)\n",
    "        score_folds = score_folds + cohen_kappa_score(y_train.iloc[oof_index], \n",
    "                                                            lgb_model.predict(X_train.iloc[oof_index]).argmax(axis=1),weights = 'quadratic')/n_splits\n",
    "\n",
    "\n",
    "    #Guardo prediccion del trial sobre el conjunto de test\n",
    "    # Genero nombre de archivo\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    # Copia del dataset para guardar la prediccion\n",
    "    predicted_df = test.copy()\n",
    "    # Genero columna pred con predicciones sumadas de los 5 folds\n",
    "    predicted_df['pred'] = [scores_ensemble[p,:] for p in range(scores_ensemble.shape[0])]\n",
    "    # Grabo dataframe en temp_artifacts\n",
    "    dump(predicted_df, predicted_filename)\n",
    "    # Indico a optuna que asocie el archivo generado al trial\n",
    "    upload_artifact(trial, predicted_filename, artifact_store)    \n",
    "\n",
    "    #Grabo natriz de confusion\n",
    "    #Nombre de archivo\n",
    "    cm_filename = os.path.join(PATH_TO_TEMP_FILES,f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "    #Grabo archivo\n",
    "    plot_confusion_matrix(y_test,scores_ensemble.argmax(axis=1)).write_image(cm_filename)\n",
    "    #Asocio al trial\n",
    "    upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "    #Determino score en conjunto de test y asocio como metrica adicional en optuna\n",
    "    test_score = cohen_kappa_score(y_test,scores_ensemble.argmax(axis=1),weights = 'quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    #Devuelvo score del 5fold cv a optuna para que optimice en base a eso\n",
    "    return(score_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_Base_30.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_Base_30_CV\",\n",
    "                            load_if_exists = True)\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
