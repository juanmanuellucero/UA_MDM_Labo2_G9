{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c37b40",
   "metadata": {},
   "source": [
    "## Configuracion de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3edd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si estás en Jupyter Notebook, anclás a una ruta conocida o fija:\n",
    "project_root = r\"C:\\Users\\juanm\\GitHub\\UA_MDM_Labo2_G9\"  # ⚠️ Personalizá esto\n",
    "\n",
    "# Rutas de trabajo\n",
    "workDir = os.path.join(project_root, \"work\")\n",
    "optunaArtifactDir = os.path.join(workDir, \"optuna_artifacts\")\n",
    "optunaTempDir = os.path.join(workDir, \"optuna_temp_artifacts\")\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(optunaArtifactDir, exist_ok=True)\n",
    "os.makedirs(optunaTempDir, exist_ok=True)\n",
    "\n",
    "# Cambiar al directorio de trabajo solo si es necesario\n",
    "if os.getcwd() != workDir:\n",
    "    os.chdir(workDir)\n",
    "\n",
    "print(\"Directorio actual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelado: Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Sklearn: splits, métricas y utilidades\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt  # Usar junto con seaborn o plotly si es necesario\n",
    "import plotly.express as px      # Corrección: no usar `from plotly import express as px`\n",
    "\n",
    "# Optuna para optimización de hiperparámetros\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "# Utilidades propias\n",
    "from utiles import plot_confusion_matrix\n",
    "\n",
    "# Guardado de objetos\n",
    "from joblib import dump, load\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b908605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths para acceso archivos\n",
    "#Este notebook asume la siguiente estructura de carpetas a partir de la ubicacion de base_dir \n",
    "#(dos niveles arriba de la cƒarpeta donde se ejecuta el notebook). \n",
    "# /ƒ/ƒ\n",
    "# /UA_MDM_Labo2/inputƒ\n",
    "# /UA_MDM_Labo2/input/petfinder-adoption-prediction/            <- Aca deben ir todos los archivos de datos de la competencia \n",
    "# /UA_MDM_Labo2/tutoriales/                       <- Aca deben poner los notebooks y scripts que les compartimos\n",
    "# /UA_MDM_Labo2/work/                             <- Resultados de notebooks iran dentro de esta carpeta en subcarpetas\n",
    "# /UA_MDM_Labo2/work/models/                     <- Modelos entrenados en archivos joblibs\n",
    "# /UA_MDM_Labo2/work/optuna_temp_artifacts/      <- Archivos que queremos dejar como artefacto de un trial de optuna (optuna los copiara a la carpeta de abajo)\n",
    "# /UA_MDM_Labo2/work/optuna_artifacts/           <- Archivos con artefactos que sibimos a optuna\n",
    "\n",
    "#Subimos dos niveles para quedar en la carpeta que contiene input y UA_MDM_Labo2\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#Datos de entrenamiento \n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "\n",
    "#Datos de razas \n",
    "PATH_TO_BREED_LABELS = os.path.join(BASE_DIR, \"modelo/data/petfinder-adoption-prediction/BreedLabels.csv\")\n",
    "\n",
    "#Salida de modelos entrenados\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"work/models\")\n",
    "\n",
    "#Artefactos a subir a optuna\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "\n",
    "#Artefactos que optuna gestiona\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "\n",
    "SEED = 42 #Semilla de procesos aleatorios (para poder replicar exactamente al volver a correr un modelo)\n",
    "TEST_SIZE = 0.2 #Facción para train/test= split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e967956",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd789d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# -------------------------\n",
    "# Feature Engineering: pelo y raza\n",
    "# -------------------------\n",
    "\n",
    "# Completo valores faltantes en FurLength\n",
    "dataset['FurLength'] = dataset['FurLength'].fillna(0).astype(int)\n",
    "\n",
    "# Merge para agregar los nombres de las razas\n",
    "breed_labels = pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "breed_dict = breed_labels.set_index('BreedID')['BreedName'].to_dict()\n",
    "\n",
    "dataset['BreedName1'] = dataset['Breed1'].map(breed_dict)\n",
    "dataset['BreedName2'] = dataset['Breed2'].map(breed_dict).fillna('')\n",
    "\n",
    "# Combinación de razas\n",
    "dataset['BreedGroup'] = dataset['BreedName1'] + \"_\" + dataset['BreedName2']\n",
    "\n",
    "# Codificación numérica de la combinación de razas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset['BreedGroupEnc'] = le.fit_transform(dataset['BreedGroup'])\n",
    "\n",
    "# Raza cruzada\n",
    "dataset['MixedBreed'] = (dataset['Breed2'] != 0).astype(int)\n",
    "\n",
    "# Interacción entre pelo y raza\n",
    "dataset['FurBreedCombo'] = dataset['FurLength'].astype(str) + \"_\" + dataset['BreedGroupEnc'].astype(str)\n",
    "dataset['FurBreedCombo'] = LabelEncoder().fit_transform(dataset['FurBreedCombo'])\n",
    "\n",
    "# Frecuencia de la raza en el dataset\n",
    "raza_freq = dataset['BreedGroup'].value_counts().to_dict()\n",
    "dataset['BreedFreq'] = dataset['BreedGroup'].map(raza_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo un 20% para test estratificado opr target\n",
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = dataset.AdoptionSpeed)\n",
    "\n",
    "#Armo listas con features de texto y numericas\n",
    "char_feats = [f for f in dataset.columns if dataset[f].dtype=='O']\n",
    "numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']\n",
    "\n",
    "#Defino features a usar\n",
    "features = numeric_feats.copy()\n",
    "\n",
    "label = 'AdoptionSpeed'\n",
    "\n",
    "# Eliminamos 'AdoptionSpeed' si está\n",
    "if 'AdoptionSpeed' in features:\n",
    "    features.remove('AdoptionSpeed')\n",
    "\n",
    "#Genero dataframes de train y test con sus respectivos targets\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca5d89",
   "metadata": {},
   "source": [
    "## Modelo con hiperparametros por default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cacb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entreno un modelo inicial sin modificar hiperparametros. Solamente especifico el numero de clases y el tipo de modelo como clasificacoión\n",
    "lgb_params = params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y_train.unique())\n",
    "                        }\n",
    "\n",
    "\n",
    "#genero el objeto Dataset que debo pasarle a lightgbm para que entrene\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "#entreno el modelo con los parametros por defecto\n",
    "lgb_model1 = lgb.train(lgb_params,\n",
    "                      lgb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model1.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0683698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo las predicciones sobre el set de test. El modelo me da una lista de probabilidades para cada clase y tomo la clase con mayor probabilidad con la funcion argmax\n",
    "y_pred = lgb_model1.predict(X_test).argmax(axis=1)\n",
    "\n",
    "#Calculo el Kappa\n",
    "print(cohen_kappa_score(y_test,y_pred, weights = 'quadratic'))\n",
    "\n",
    "#Muestro la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270bbba",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que vamos a optimizar. Optuna requiere que usemos el objeto trial para generar los parametros a optimizar\n",
    "def lgb_objective(trial):\n",
    "    #PArametros para LightGBM\n",
    "    lgb_params = {      \n",
    "                        #PArametros fijos\n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique()),\n",
    "                        #Hiperparametros a optimizar utilizando suggest_float o suggest_int segun el tipo de dato\n",
    "                        #Se indica el nombre del parametro, valor minimo, valor maximo \n",
    "                        #en elgunos casos el parametro log=True para parametros que requieren buscar en esa escala\n",
    "                        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                        } \n",
    "\n",
    "    #Genero objeto dataset de entrenamiento\n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                    label=y_train)\n",
    "\n",
    "    #ajuste de modelo\n",
    "    lgb_model2 = lgb.train(lgb_params,\n",
    "                        lgb_train_dataset)\n",
    "    \n",
    "    #Devuelvo el score en test\n",
    "    return(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el estudio a optimizar\n",
    "study = optuna.create_study(direction='maximize', #buscamos maximizar la metrica\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_Grupal_30.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"100_LGBM_Grupal_30\", #nombre del experimento\n",
    "                            load_if_exists=True) #continuar si ya existe\n",
    "\n",
    "#Corremos 100 trials para buscar mejores parametros\n",
    "study.optimize(lgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos mejor resultado\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a replicar el resultado de la optimizacion reentrenando el modelo con el mejor conjunto de hiperparametros\n",
    "#Generamos parametros incluyendo los fijos y la mejor solución que encontro optuna\n",
    "lgb_params =  {      \n",
    "                        'objective': 'multiclass',\n",
    "                        'verbosity':-1,\n",
    "                        'num_class': len(y_train.unique())} | study.best_params\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train,\n",
    "                                label=y_train)\n",
    "\n",
    "\n",
    "#Entreno\n",
    "lgb_model2 = lgb.train(lgb_params,\n",
    "                    lgb_train_dataset)\n",
    "\n",
    "#Muestro matriz de confusion y kappa\n",
    "print(cohen_kappa_score(y_test,lgb_model2.predict(X_test).argmax(axis=1),\n",
    "                             weights = 'quadratic'))\n",
    "\n",
    "display(plot_confusion_matrix(y_test,lgb_model2.predict(X_test).argmax(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52853014",
   "metadata": {},
   "source": [
    "## Modelo con optimizacion de hiperparametros con 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(), dy_pred.argmax(axis=1), weights='quadratic')\n",
    "    is_higher_better = True\n",
    "    return (metric_name, value, is_higher_better)\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    \n",
    "    # Inicio el store de artefactos (archivos) de Optuna\n",
    "    artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "    # Hiperparámetros a optimizar\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    scores_ensemble = np.zeros((len(y_test), len(y_train.unique())))\n",
    "    score_folds = 0\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=False)\n",
    "        \n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=False)\n",
    "                \n",
    "        lgb_model3 = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_if_dataset,\n",
    "            valid_sets=lgb_oof_dataset,\n",
    "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "            feval=lgb_custom_metric_kappa\n",
    "        )\n",
    "        \n",
    "        scores_ensemble += lgb_model3.predict(X_test)\n",
    "\n",
    "        score_folds += cohen_kappa_score(\n",
    "            y_train.iloc[oof_index],\n",
    "            lgb_model3.predict(X_train.iloc[oof_index]).argmax(axis=1),\n",
    "            weights='quadratic'\n",
    "        ) / n_splits\n",
    "    \n",
    "    # Guardar predicciones sobre test\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    predicted_df = test.copy()\n",
    "    predicted_df['pred'] = [scores_ensemble[p, :] for p in range(scores_ensemble.shape[0])]\n",
    "    dump(predicted_df, predicted_filename)\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar las predicciones como artefacto.\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=predicted_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "\n",
    "    # Generar nombre de archivo (¡esto es un string, no lo pises!)\n",
    "    cm_filename = os.path.join(PATH_TO_TEMP_FILES, f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "\n",
    "    # Crear y guardar matriz de confusión\n",
    "    cm = confusion_matrix(y_test, scores_ensemble.argmax(axis=1))\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot()\n",
    "    plt.savefig(cm_filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Uso keyword arguments en upload_artifact. Asociar la imagen como artefacto\n",
    "    upload_artifact(\n",
    "        study_or_trial=trial,\n",
    "        file_path=cm_filename,\n",
    "        artifact_store=artifact_store\n",
    "    )\n",
    "    \n",
    "    # Guardar el score en test como métrica auxiliar\n",
    "    test_score = cohen_kappa_score(y_test, scores_ensemble.argmax(axis=1), weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    # Devuelvo el promedio de los scores del CV (objetivo a maximizar)\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db_100_LGBM_Grupal_30.sqlite3\",\n",
    "                            study_name=\"100_LGBM_Grupal_30_CV\",\n",
    "                            load_if_exists = True)\n",
    "\n",
    "#Corro la optimizacion\n",
    "study.optimize(cv_es_lgb_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a5d3b",
   "metadata": {},
   "source": [
    "## LGBM Importance Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "features = np.array(features)\n",
    "model= lgb_model2\n",
    "\n",
    "# --------------------\n",
    "# 1. Feature Importance basada en GAIN\n",
    "# --------------------\n",
    "importances_gain = model.feature_importance(importance_type='gain')\n",
    "sorted_idx_gain = importances_gain.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_gain], importances_gain[sorted_idx_gain])\n",
    "plt.xlabel('Importancia (GAIN)')\n",
    "plt.title('Feature Importance basada en GAIN')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 2. Feature Importance basada en SPLIT\n",
    "# --------------------\n",
    "importances_split = model.feature_importance(importance_type='split')\n",
    "sorted_idx_split = importances_split.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_split], importances_split[sorted_idx_split])\n",
    "plt.xlabel('Importancia (SPLIT)')\n",
    "plt.title('Feature Importance basada en SPLIT')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 3. Permutation Importance\n",
    "# --------------------\n",
    "# Para permutation necesitamos un modelo tipo sklearn, armamos uno rápido\n",
    "model_sklearn = lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "perm_result = permutation_importance(model_sklearn, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "sorted_idx_perm = perm_result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(features[sorted_idx_perm], perm_result.importances_mean[sorted_idx_perm])\n",
    "plt.xlabel('Impacto en Accuracy')\n",
    "plt.title('Permutation Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 4. SHAP Values\n",
    "# --------------------\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Graficar importancia promedio\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features, plot_type=\"dot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
