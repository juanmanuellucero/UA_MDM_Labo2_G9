{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuentes: https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalar Modulos**\n",
    "\n",
    "conda install datasets==\"2.20.0\"\n",
    "\n",
    "conda install transformers==\"4.40.1\"\n",
    "\n",
    "conda install numpy==\"1.26.4\" # La última versión no funciona bien\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "from datasets import Dataset,  DatasetDict\n",
    "\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "# Modeling\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils import plot_confusion_matrix, get_artifact_filename\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "# Verificamos que CUDA está funcional\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bajamos el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Armado de los Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = '../'\n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"work/optuna_temp_artifacts\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"work/optuna_artifacts\")\n",
    "\n",
    "# Parametros y variables\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MODEL_NAME = '06 Bert'\n",
    "\n",
    "MODEL_VERSION = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 19:53:19,385] Using an existing study with name '04 - LGB Multiclass CV' instead of creating a new one.\n",
      "Stringifying the column: 100%|██████████| 11984/11984 [00:00<00:00, 449035.09 examples/s]\n",
      "Casting to class labels: 100%|██████████| 11984/11984 [00:00<00:00, 413655.65 examples/s]\n",
      "Stringifying the column: 100%|██████████| 2996/2996 [00:00<00:00, 374114.58 examples/s]\n",
      "Casting to class labels: 100%|██████████| 2996/2996 [00:00<00:00, 304029.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Name', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID', 'VideoAmt', 'Description', 'PetID', 'PhotoAmt', 'AdoptionSpeed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv(PATH_TO_TRAIN)\n",
    "df = df[df['Description'].notnull()]\n",
    "df['labels'] = df[\"AdoptionSpeed\"]\n",
    "\n",
    "# Dividir los datos usando sklearn\n",
    "#train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=SEED, stratify=df.AdoptionSpeed)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=\"04 - LGB Multiclass CV\",\n",
    "                           load_if_exists = True)\n",
    "\n",
    "lgb_test_dataset = load(os.path.join(PATH_TO_OPTUNA_ARTIFACTS,get_artifact_filename(study_lgb,'test')))\n",
    "\n",
    "train_df = df[~df.PetID.isin(lgb_test_dataset.PetID)].reset_index(drop=True)\n",
    "test_df = df[df.PetID.isin(lgb_test_dataset.PetID)].reset_index(drop=True)\n",
    "\n",
    "# Convertir a Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combinar en un DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': test_dataset\n",
    "})\n",
    "\n",
    "# Codificar la columna de etiquetas como clases\n",
    "dataset = dataset.class_encode_column('labels')\n",
    "\n",
    "# Hacer una lista de columnas para remover antes de la tokenización\n",
    "cols_to_remove = [col for col in dataset[\"train\"].column_names if col != 'labels']\n",
    "print(cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/11984 [00:00<?, ? examples/s]/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=4): 100%|██████████| 11984/11984 [00:01<00:00, 6421.50 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/2996 [00:00<?, ? examples/s]/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=4): 100%|██████████| 2996/2996 [00:00<00:00, 4877.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and encode the dataset\n",
    "def tokenize(batch):\n",
    "    from transformers import DistilBertTokenizerFast\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    tokenized_batch = tokenizer(batch[\"Description\"], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_batch\n",
    "\n",
    "dataset_enc = dataset.map(tokenize, batched=True, remove_columns=cols_to_remove, num_proc=4)\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "dataset_enc.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Check the output\n",
    "print(dataset_enc[\"train\"].column_names)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a data collator with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create data loaders for to reshape data for PyTorch model\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_enc[\"train\"], shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset_enc[\"val\"], batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_ids =[i for i in test_df.PetID] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dynamically set number of class labels based on dataset\n",
    "num_labels = dataset[\"train\"].features['labels'].num_classes\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set the device automatically (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, dataloaders, datasets, device, num_epochs=4, lr=0.001, trial=None):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Further define learning rate scheduler\n",
    "    num_training_batches = len(train_dataloader)\n",
    "    num_training_steps = num_epochs * num_training_batches\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",                   # linear decay\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_kappa =  -999\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    try:\n",
    "        previous_best = study.best_value\n",
    "    except:\n",
    "        previous_best = -999\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        kappa_labels_true = []\n",
    "        kappa_labels_predicted = []\n",
    "        output_scores = []\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch in tqdm(dataloaders[phase]):\n",
    "                batch = batch.to(device)\n",
    "                #inputs = inputs.to(device)\n",
    "                labels = batch.labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    preds_labels = torch.argmax(preds, dim=-1)\n",
    "\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    elif phase == 'val':\n",
    "                        kappa_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "                        kappa_labels_predicted.extend(preds_labels.cpu().numpy().tolist())\n",
    "                        outputs_np = preds.cpu().numpy()\n",
    "                        output_scores.extend([outputs_np[i,:] for i in range(outputs_np.shape[0])])\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                running_corrects += torch.sum(preds_labels == labels.data)\n",
    "                \n",
    "                #END OF BATCH\n",
    "\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(datasets[phase])\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                kappa_score = np.nan\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                kappa_score = cohen_kappa_score(kappa_labels_true,\n",
    "                                  kappa_labels_predicted,\n",
    "                                  weights = 'quadratic')\n",
    "                    \n",
    "\n",
    "\n",
    "            print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc*100:.2f}% Kappa: {kappa_score:.3f}')\n",
    "\n",
    "            # If this is the best Epoch so far -> Deep copy the model\n",
    "            if phase == 'val' and kappa_score > best_kappa:\n",
    "                best_acc = epoch_acc\n",
    "                best_kappa = kappa_score\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "                #Best Epoch within a trial and better than previous trials\n",
    "                if trial is not None and best_kappa > previous_best:\n",
    "\n",
    "                    #Save test dataset with predictions\n",
    "                    predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "                    predicted_df = pd.DataFrame({'PetID':test_sample_ids,\n",
    "                                'pred':output_scores}).merge(test_df, on='PetID')\n",
    "                    dump(predicted_df, predicted_filename)\n",
    "\n",
    "                    #Generate and save CM \n",
    "                    cm_filename = os.path.join(PATH_TO_TEMP_FILES,f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "                    plot_confusion_matrix(kappa_labels_true,kappa_labels_predicted).write_image(cm_filename)\n",
    "\n",
    "            #END OF PHASE\n",
    "\n",
    "        #END OF EPOCH\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(best_acc * 100))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save in optuna trial the best test dataset, cm and model weights\n",
    "    if trial is not None and best_kappa > previous_best:\n",
    "        upload_artifact(trial, predicted_filename, artifact_store)   \n",
    "\n",
    "        upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "        file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{trial.number}.pth'\n",
    "        model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "        torch.save(model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "        upload_artifact(trial, model_path, artifact_store)\n",
    "\n",
    "    return model,best_kappa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dynamically set number of class labels based on dataset\n",
    "num_labels = dataset[\"train\"].features['labels'].num_classes\n",
    "print(f\"Number of labels: {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:04<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4527 Acc: 31.19% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4241 Acc: 34.58% Kappa: 0.128\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3797 Acc: 37.33% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3992 Acc: 36.68% Kappa: 0.204\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2412 Acc: 46.23% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4458 Acc: 36.92% Kappa: 0.226\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9928 Acc: 58.66% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6550 Acc: 37.15% Kappa: 0.223\n",
      "Epoch 4/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7063 Acc: 71.95% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.0008 Acc: 33.71% Kappa: 0.213\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4922 Acc: 81.17% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.2346 Acc: 35.71% Kappa: 0.215\n",
      "Epoch 6/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3659 Acc: 86.32% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.4345 Acc: 36.42% Kappa: 0.222\n",
      "Epoch 7/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2576 Acc: 90.45% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.7661 Acc: 35.65% Kappa: 0.228\n",
      "Epoch 8/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2121 Acc: 91.91% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.0521 Acc: 35.11% Kappa: 0.202\n",
      "Epoch 9/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1730 Acc: 93.67% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.0954 Acc: 36.72% Kappa: 0.238\n",
      "Epoch 10/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1674 Acc: 93.79% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.0650 Acc: 35.45% Kappa: 0.227\n",
      "Epoch 11/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1486 Acc: 94.42% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.1498 Acc: 36.75% Kappa: 0.224\n",
      "Epoch 12/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1378 Acc: 94.68% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.2532 Acc: 36.48% Kappa: 0.230\n",
      "Epoch 13/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1329 Acc: 94.69% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.2980 Acc: 35.31% Kappa: 0.222\n",
      "Epoch 14/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1217 Acc: 95.27% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.3602 Acc: 36.75% Kappa: 0.215\n",
      "Training complete in 34m 30s\n",
      "Best val Acc: 36.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model,_ = train_val(model,\n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': eval_dataloader}, \n",
    "                       datasets=dataset_enc, \n",
    "                       device=device, \n",
    "                       lr = 5e-5,\n",
    "                       num_epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ../work/optuna_temp_artifacts/06 Bert_1.0_20250424_202754.pth\n"
     ]
    }
   ],
   "source": [
    "# Guardo el modelo\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{run_id}.pth'\n",
    "model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "torch.save(best_model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "print(f'Modelo guardado en {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71841/1390831205.py:1: ExperimentalWarning: FileSystemArtifactStore is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "  artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n"
     ]
    }
   ],
   "source": [
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "\n",
    "def optuna_train(trial):\n",
    "\n",
    "    epochs = trial.suggest_int('epochs', 1, 2)\n",
    "\n",
    "    lr = trial.suggest_float('lr', 0.00001, 0.0001, log=True)\n",
    "\n",
    "    _,best_score = train_val(model, \n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': eval_dataloader}, \n",
    "                       datasets=dataset_enc, \n",
    "                       device=device, \n",
    "                       num_epochs=epochs,\n",
    "                       lr=lr,\n",
    "                       trial=trial)\n",
    "\n",
    "\n",
    "    return(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 20:27:54,822] A new study created in RDB with name: 06 Bert_1.0\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1173 Acc: 95.36% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.2440 Acc: 35.81% Kappa: 0.222\n",
      "Training complete in 2m 26s\n",
      "Best val Acc: 35.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71841/1381733137.py:135: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_71841/1381733137.py:137: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_71841/1381733137.py:142: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "[I 2025-04-24 20:30:21,652] Trial 0 finished with value: 0.22166862844507484 and parameters: {'epochs': 1, 'lr': 1.2889027985365552e-05}. Best is trial 0 with value: 0.22166862844507484.\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1435 Acc: 94.66% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "[I 2025-04-24 20:32:39,591] Trial 1 finished with value: 0.2087732368116748 and parameters: {'epochs': 1, 'lr': 4.454059529458674e-05}. Best is trial 0 with value: 0.22166862844507484.\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.2626 Acc: 34.58% Kappa: 0.209\n",
      "Training complete in 2m 18s\n",
      "Best val Acc: 34.58%\n",
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0972 Acc: 96.24% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.4492 Acc: 36.25% Kappa: 0.243\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0834 Acc: 96.47% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "/tmp/ipykernel_71841/1381733137.py:135: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_71841/1381733137.py:137: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.5039 Acc: 36.25% Kappa: 0.246\n",
      "Training complete in 4m 37s\n",
      "Best val Acc: 36.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71841/1381733137.py:142: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "[I 2025-04-24 20:37:16,944] Trial 2 finished with value: 0.24564131186114824 and parameters: {'epochs': 2, 'lr': 1.133490942520377e-05}. Best is trial 2 with value: 0.24564131186114824.\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0908 Acc: 96.41% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.4675 Acc: 35.95% Kappa: 0.219\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0810 Acc: 96.63% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "[I 2025-04-24 20:41:53,353] Trial 3 finished with value: 0.2217508660430798 and parameters: {'epochs': 2, 'lr': 1.8825720477791556e-05}. Best is trial 2 with value: 0.24564131186114824.\n",
      "/home/aa/miniconda3/envs/ldi2_cuda_trf/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.6092 Acc: 36.58% Kappa: 0.222\n",
      "Training complete in 4m 36s\n",
      "Best val Acc: 36.58%\n",
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1939 Acc: 92.65% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.2824 Acc: 34.75% Kappa: 0.198\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2109 Acc: 91.97% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "[I 2025-04-24 20:46:29,646] Trial 4 finished with value: 0.20642553260851848 and parameters: {'epochs': 2, 'lr': 9.47255212993548e-05}. Best is trial 2 with value: 0.24564131186114824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.9462 Acc: 34.85% Kappa: 0.206\n",
      "Training complete in 4m 36s\n",
      "Best val Acc: 34.85%\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///../work/db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=f'{MODEL_NAME}_{MODEL_VERSION}',\n",
    "                            load_if_exists = True)\n",
    "study.optimize(optuna_train, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldi2_cuda_trf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
